<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="description" content="Neural Brushstroke Engine, a GAN-based technique to learn a latent space
  of interactive digital drawing tools from unlabeled images from NVIDIA Toronto AI Lab">
  <meta name="keywords"
  content="Generative adversarian network, GAN, digital painting, ">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet" />
  <link href="assets/style.css?v=0.2" rel='stylesheet' type='text/css' />
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css' />
  <title>
    Neural Brushstroke Engine: Learning a Latent Style Space of Interactive Drawing Tools
</title>
</head>

<body>
    <div class="topnav" id="myTopnav">
        <div>
          <a href="https://www.nvidia.com/"><img width="100%" src="assets/nvidia.svg" /></a>
          <a href="https://nv-tlabs.github.io/"><strong>Toronto AI Lab</strong></a>
      </div>
  </div>
  <div class="title">
    <h1>
      Neural Brushstroke Engine: Learning a Latent Style Space of Interactive Drawing Tools
  </h1>
</div>
<div class="authors" , style="text-align: center;">
    <a href="https://shumash.com/" target="_blank">Masha Shugrina<sup>1</sup></a>
    <a href="https://www.linkedin.com/in/chin-ying-li/" target="_blank">Chin-Ying Li<sup>2</sup></a>
    <a href="https://www.cs.toronto.edu/~fidler/" target="_blank">Sanja Fidler<sup>1,3,4</sup></a>
</div>
<div class="affil" , style="text-align: center;">
    <span><sup>1</sup>NVIDIA</span>
    <span><sup>2</sup>Asana</span>
    <span><sup>2</sup>University of Toronto</span>
    <span><sup>3</sup>Vector Institute</span>
</div>

<div class="affil-row">
    <div class="venue text-center"><b>SIGGRAPH Asia 2022</b></div>
</div>

<div style="clear: both">
    <div class="paper-btn-parent">
        <a class="supp-btn" href="https://drive.google.com/file/d/1RNFgMXEp85MGlV6w99JC_MBBg45TFgzI/view?usp=sharing">
            <span class="material-icons"> description </span> 
            Paper
        </a>

        <a class="supp-btn" href="https://drive.google.com/file/d/1cxr-g0_pWW-py5ySzkeJ1lFVl5HjDfRK/view?usp=sharing">
            <span class="material-icons"> folder </span> 
            Suppl
        </a>

        <a class="supp-btn" href="#bibtex">
            <span class="material-icons"> bookmark </span> 
            BibTex
        </a>

        <a class="supp-btn" href="astro_dataset.html">
            <span class="material-icons"> dataset </span> 
            Data
        </a>

        <!--<a class="supp-btn" href="assets/bib.txt">
            <span class="material-icons"> brush </span> 
            Demo
        </a>-->

        <a class="supp-btn" href="https://github.com/nv-tlabs/brushstroke_engine">
            <span class="material-icons"> code </span> 
            Code
        </a>
    </div>
</div>

<section id="teaser">
    <figure style="width: 100%;">
        <video class="centered" width="90%" muted loop autoplay>
            <source src="assets/teaser_opt.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>

    </figure>
</section>

<section>
    <h2>Abstract</h2>
    <hr>
    <p>
        Neural Brushstroke Engine (NeuBE) includes a GAN model that learns to mimic many drawing media styles by learning from unlabeled images. The result is a GAN model that can be directly controlled by user strokes, with style code <b>z</b> corresponding to the style of the interactive brush (and not the final image). Together with a patch-based paining engine, NeuBE allows seamless drawing on a canvas of any size -- in a wide variety of learned natural and novel AI brush styles. Our model can be trained on only about 200 images of drawing media, is shown to match the training styles well, and generalizes to unseen out-of-distribution styles. This invites novel applications, like text-based retrieval of brushes and matching brushes to existing art to allow interactive painting in that style. Our generator supports user control of stroke color for any brush style and compositing of strokes on clear background. We also support automatic stylization of line drawings. Code, data and demo are coming soon.
    </p>
</section>

<section>
    <h2>Drawing with AI Brushes</h2>
    <hr>
    <p>
        NeuBE's continuous latent space of brushes allows text-based search over interactive tools for the first time.
        By leveraging <a href="https://github.com/openai/CLIP">CLIP</a> embedding space our system can discover interactive brush styles very different from the training data.  
    </p>
    <figure style="width: 100%;">
        <video class="centered" width="80%" controls muted loop autoplay>
            <source src="assets/web_clipdraw.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
    </figure>
</section>

<section>
    <h2>Method Overview</h2>
    <hr>
    <p>
        To approximate a distribution of interactive drawing styles, NeuBE is trained on a style dataset of unlabeled scribbles in many
        target media and an unpaired geometry dataset of plain black on white strokes, generated synthetically. 
        NeuBE's patch-based generator extends <a href="https://github.com/NVlabs/stylegan2-ada-pytorch">StyleGAN2</a> architecture (striped yellow blocks) to condition image generation on stroke geometry g. Represented as a binary image, g is passed through a pre-trained Geometry Encoder to produce spatial features that are concatenated with the StyleGAN features. Instead of outputting RGB, the final ToTriad layer of our generator produces a decomposed image representation that allows recoloring, compositing and formulating a geometric loss <i>Lùëîùëíùëú</i>. 
    </p>
    <figure>
        <img class="centered" width="100%" src="assets/arch_v2.jpeg" />
    </figure>
    <p>
        Instead of RGB images, the ToTriad layer outputs <b>Alpha-color (AC) parametrization</b>: 3 colors and 3 spatial alpha maps that can be
        deterministically composited to produce the final image. This allows us to support user control of color for any brush style <b>z</b> as well as geometric control and compositing of clear strokes. 
    </p>
    <figure>
        <div class="half gc">
            <div><b>Geometric Control</b></div>
            <img src="assets/control2.jpeg"/>
        </div>
        <div class="half colc">
            <div><b>Color Control</b></div>
            <img src="assets/control_color2.jpeg"/>
        </div>
    </figure>
    <p>
        Together with a carefully-designed painting engine, the generator backbone allows NeuBE to support interactive drawing
        in a wide variety of learned styles on a canvas of any size by generating on the patch level. Our demos are implemented as a client-server web-based demo, with the generator running
        on a remote GPU server and the drawing interface operating in the browser on another machine or tablet.
    </p>
    <figure>
        <img class="centered" width="100%" src="assets/client_server.jpeg" />
    </figure>
</section>

<section>
    <h2>Automatic Sketch Stylization</h2>
    <hr>
    <p>
        In addition to interactive drawing, NeuBE supports automatic stylization of line drawings in any of
        the learned styles by running the painting engine on the source broken up into patches, as show below. Interpolation
        of styles for these stylizations is also possible (see styles labeled interp.), allowing smooth style transitions for any line drawing.
    </p>
    <figure>
        <img class="centered" width="100%" src="assets/interp.jpeg" />
    </figure>
</section>

<section>
    <h2>Random Brush Styles</h2>
    <hr>
    <p>
        Here are some random brush styles learned by NeuBE on our Style Dataset 1, comprised of 148 informally captured scribbles of natural media. All drawing is done interactively.
    </p>
    <figure>
        <video class="centered" width="80%" controls muted loop autoplay>
            <source src="assets/web_random.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
    </figure>
</section>

<!--<section>
    <h2>Matching Training Styles</h2>
    <hr>
    <p>
        When embedded into the latent W+ space, trainin styles are accurately represented as interactive brushes. 
    </p>
    <figure>
        <img class="centered" width="100%" src="assets/embed_results2.jpeg" />
    </figure>
</section>-->

<section>
    <h2>Generalization to Unseen Styles</h2>
    <hr>
    <p>
        When embedded into the latent W+ space, training examples are accurately represented as interactive brushes. 
        In addition, embedding styles from unseen datasets shows excellent generalization to all but the most challenging 
        of styles.
    </p>
    <figure>
        <img class="centered" width="100%" src="assets/embed_results_gen2.jpeg" />
    </figure>
</section>

<section>
    <h2>CLIP Brush Search</h2>
    <hr>
    <p>
        Generalization of the latent space to unseen styles allows novel applications for brush search and discovery in
        the latent space of drawing tools. For example, we use CLIP to search for a brush, given text. Here are some sample
        discovered styles. 
    </p>
    <figure>
        <img class="centered" width="100%" src="assets/clip_style2_hr.jpeg" />
    </figure>
</section>

<section>
    <h2>Brushes that Match Existing Art</h2>
    <hr>
    <p>
        We further probe optimizing the brush latent code of a pre-trained model to match an artwork. New art can then be 
        <i>interactively</i> drawn by the user in the style of the original. Our preliminary results show that NeuBE's latent
        space can faithfully replicate textural details of styles that fail to be accurately matched by commercial digital drawing
        applications. We are excited to see further work in this area. 
    </p>
    <figure>
        <img class="centered" width="100%" src="assets/art_embed_leaf.jpeg" />
    </figure>
</section>

<section>
    <h2>More Interactive Demos</h2>
    <hr>
    <p>
        Drawing in our web-based user interface. Drawing is performed in the Safari browser on an iPad device with apple pencil, while the GAN generator is running on the server. See additional high-resolution drawings: 
        <a href="assets/interact_burnt.jpeg">CLIP ("burnt umber oil paint")</a>, <a href="assets/interact_candy.jpg">CLIP ("candy cane")</a>, <a href="assets/interact_playdoh.jpg">several embedded playdoh styles</a>, <a href="assets/interact_thumb.jpg">single style with two-tone color control</a>.
    </p>
    <figure>
        <video class="centered" width="80%" controls muted loop autoplay>
            <source src="assets/web_interactive2.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
    </figure>
</section>

<section>
    <h2>Technical Presentation</h2>
    <hr>
    <p>
        <div class="video-container">
            <iframe src="https://www.youtube.com/embed/rIXNXlMFZtg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
            </iframe>
    </div>
    </p>
</section>



<section>
    <h2 id="bibtex">Citation</h2>
    <hr>
    <p>
        If building on our ideas, code or data, please cite:
        <pre class="language-latex citation">
            <code class="language-latex">
@article{shugrina2022neube,
  title={Neural Brushstroke Engine: Learning a Latent Style Space of Interactive Drawing Tools},
  author={Shugrina, Maria and Li, Chin-Ying and Fidler, Sanja},
  journal={ACM Transactions on Graphics (TOG)},
  volume={41},
  number={6},
  year={2022},
  publisher={ACM New York, NY, USA}
}
                        </code>
                    </pre>
    </p>
</section>

</body>
</html>
